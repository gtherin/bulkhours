{
    "alexandra.larsonneur@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    \n    x_norm = np.linalg.norm(x,ord = 2,axis = 1,keepdims = True)\n\n    # Divide x by its norm.\n    x = x / x_norm\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n    \n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum\n\n    ### END CODE HERE ###\n    \n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "alexandra.larsonneur@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    \n    x_norm = np.linalg.norm(x,ord = 2,axis = 1,keepdims = True)\n\n    # Divide x by its norm.\n    x = x / x_norm\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n    \n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum\n\n    ### END CODE HERE ###\n    \n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:17:21",
        "atype": "bkcode"
    },
    "alexis.akujuobi-asoluka@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "alexis.akujuobi-asoluka@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 20:28:34",
        "atype": "bkcode"
    },
    "ali.m-sahi@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x_div = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "ali.m-sahi@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x_div = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:00:23",
        "atype": "bkcode"
    },
    "angel.jouen@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "angel.jouen@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:14:53",
        "atype": "bkcode"
    },
    "antoine.rochette@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "antoine.rochette@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:17:34",
        "atype": "bkcode"
    },
    "arij.salablab@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "arij.salablab@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:56:33"
    },
    "armand.loisil@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x_norm = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis=1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "armand.loisil@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x_norm = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis=1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:13:14",
        "atype": "bkcode"
    },
    "axel.autexier@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "axel.streiff@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord=2,axis=1,keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "axel.streiff@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord=2,axis=1,keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:16:10",
        "atype": "bkcode"
    },
    "badis.perdrix@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,axis=1,keepdims=True) # ...\n\n    # Divide x by its norm.\n    x_normalized = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "badis.perdrix@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,axis=1,keepdims=True) # ...\n\n    # Divide x by its norm.\n    x_normalized = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:17:44",
        "atype": "bkcode"
    },
    "baptiste.rousselet@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "colin.duchanoy@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x , axis = 1 , keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "colin.duchanoy@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x , axis = 1 , keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:06:35",
        "atype": "bkcode"
    },
    "corentin.dumont@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "dorian.rondeau@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "dorian.rondeau@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:00:53",
        "atype": "bkcode"
    },
    "evan.garcia@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum( x_exp, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "evan.garcia@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum( x_exp, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 20:06:01"
    },
    "gaetan.chiesura@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm= np.linalg.norm(x_exp, ord=2, axis=1, keepdims=True)\n    # Divide x by its norm.\n    x_normalized = x_exp/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "gaetan.chiesura@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm= np.linalg.norm(x_exp, ord=2, axis=1, keepdims=True)\n    # Divide x by its norm.\n    x_normalized = x_exp/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:16:09",
        "atype": "bkcode"
    },
    "geoffrey.vaillant@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "guillaume.therin@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "john.doe@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = 0 # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = 0 # ...\n\n    # Divide x by its norm.\n    x = 0 # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = 0 # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = 0 # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(\"softmax(x) = \" + str(softmax(x)))\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "john.doe@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = 0 # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = 0 # ...\n\n    # Divide x by its norm.\n    x = 0 # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = 0 # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = 0 # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(\"softmax(x) = \" + str(softmax(x)))\n",
        "note": 0.0,
        "update_time": "2023-09-27 15:15:43",
        "atype": "bkcode"
    },
    "julian.lavarelo@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n    n,m = np.shape(x)\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, keepdims = 1) # ...\n\n    # Divide x by its norm.\n    x = x_exp/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = 1) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\nprint(np.sum(softmax(x)))\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "julian.lavarelo@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n    n,m = np.shape(x)\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, keepdims = 1) # ...\n\n    # Divide x by its norm.\n    x = x_exp/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = 1) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\nprint(np.sum(softmax(x)))\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:16:47",
        "atype": "bkcode"
    },
    "lea.dupin@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x_exp, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x_normalized = x_exp/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_normalized, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_normalized / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "lea.dupin@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x_exp, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x_normalized = x_exp/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_normalized, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_normalized / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:26:29",
        "atype": "bkcode"
    },
    "luc.fourty@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x / x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "luc.fourty@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x / x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:23:05",
        "atype": "bkcode"
    },
    "luc.sauleau@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "luc.sauleau@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:11:58",
        "atype": "bkcode"
    },
    "ludovic.said@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "mathieu.klingler@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "mathieu.klingler@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:06:35",
        "atype": "bkcode"
    },
    "matteo.crosnier-de-bellaistre@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord = 2,axis=1,keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp,axis=1,keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "matteo.crosnier-de-bellaistre@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord = 2,axis=1,keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp,axis=1,keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:07:03",
        "atype": "bkcode"
    },
    "mikael.kealbert@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord = 2, axis=1, keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x,axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "mikael.kealbert@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord = 2, axis=1, keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x,axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 23:57:16",
        "atype": "bkcode"
    },
    "nathan.heckmann@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord=2, axis=..., keepdims=True)\n    x_norm = np.linalg.norm(x_exp, ord=2, axis=1, keepdims=True)\n\n    # Divide x_exp by its norm.\n    x_normalized = x_exp / x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis=1, keepdims=True).\n    x_sum = np.sum(x_normalized, axis=1, keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_normalized / x_sum\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "nathan.heckmann@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord=2, axis=..., keepdims=True)\n    x_norm = np.linalg.norm(x_exp, ord=2, axis=1, keepdims=True)\n\n    # Divide x_exp by its norm.\n    x_normalized = x_exp / x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis=1, keepdims=True).\n    x_sum = np.sum(x_normalized, axis=1, keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_normalized / x_sum\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:28:04",
        "atype": "bkcode"
    },
    "nils.joanne@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2,axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "nils.joanne@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2,axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x/x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:05:52",
        "atype": "bkcode"
    },
    "omar.al-hammal@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "pedro-henrique.valvezon-maldonado@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "riccardo.cecchetto@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord=2,axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x /x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp ,axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "riccardo.cecchetto@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x,ord=2,axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x /x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp ,axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:11:29",
        "atype": "bkcode"
    },
    "romain.miaux@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis=1, keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "romain.miaux@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n\n    # Divide x by its norm.\n    x = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis=1, keepdims=True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x/x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:10:39"
    },
    "solution": {
        "update_time": "2023-10-10 22:41:46",
        "visible": true,
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n, m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n, m)\n    \"\"\"\n\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 1 of x. Use np.linalg.norm(..., ord = 1, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord=1, axis=1, keepdims=True)\n    print(x)\n\n    # Divide x by its norm.\n    x /= x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x / x_sum\n\n    return s\n\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "explanation": "",
        "evaluation": "def student_evaluation_function(max_score=10, debug=False):\n    x = np.array([[9., 2, 5, 0, 0], [7, 5, 0, 0 ,0]])\n    ref_data = np.array([[9.80897665e-01, 8.94462891e-04, 1.79657674e-02, 1.21052389e-04, 1.21052389e-04],\n                         [8.78679856e-01, 1.18916387e-01, 8.01252314e-04, 8.01252314e-04, 8.01252314e-04]])\n    if np.max(np.abs(student.softmax(x)-ref_data)) < 0.001:\n        return 10\n    return 0\n\n",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n, m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n, m)\n    \"\"\"\n\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x = np.exp(x)\n\n    # Compute x_norm as the norm 1 of x. Use np.linalg.norm(..., ord = 1, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord=1, axis=1, keepdims=True)\n    print(x)\n\n    # Divide x by its norm.\n    x /= x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x / x_sum\n\n    return s\n\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "user": "solution",
        "note": 10.0,
        "atype": "bkcode",
        "hint": ""
    },
    "sualp.komurcuoglu@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x / x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "sualp.komurcuoglu@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x = x / x_norm # ...\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 17:59:12"
    },
    "thibaud.bonnet@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x_exp, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x_exp / x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "thibaud.bonnet@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x_exp, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x_exp / x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:13:39",
        "atype": "bkcode"
    },
    "wassil.amghar@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x_normalized = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "wassil.amghar@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    x_exp = np.exp(x) # ...\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True) # ...\n\n    # Divide x by its norm.\n    x_normalized = x/x_norm\n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True) # ...\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp/x_sum # ...\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:15:12",
        "atype": "bkcode"
    },
    "wenceslas.lombard@ipsa.fr": {
        "note_upd": "2023-10-10 20:41:54",
        "note": NaN
    },
    "yann-loic-atasse.atakoui@ipsa.fr": {
        "answer": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    n, m = x.shape \n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x / x_norm \n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note_upd": "2023-10-10 20:41:54",
        "user": "yann-loic-atasse.atakoui@ipsa.fr",
        "main_execution": "def softmax(x):\n    \"\"\"Calculates the softmax for each row of the input x.\n\n    Your code should work for a row vector and also for matrices of shape (n, m).\n\n    Argument:\n    x -- A numpy matrix of shape (n,m)\n\n    Returns:\n    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Apply exp() element-wise to x. Use np.exp(...).\n    n, m = x.shape \n    x_exp = np.exp(x)\n\n    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n\n    # Divide x by its norm.\n    x = x / x_norm \n\n    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n\n    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n    s = x_exp / x_sum\n    ### END CODE HERE ###\n\n    return s\n\nx = np.array([[9., 2, 5, 0, 0],[7, 5, 0, 0 ,0]])\nprint(f\"softmax({x}) = {softmax(x)}\")\n",
        "note": 10.0,
        "update_time": "2023-10-03 14:22:14",
        "atype": "bkcode"
    }
}