{
    "alexandra.larsonneur@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A = sigmoid(np.dot(w.T,X) + b)                                     # compute activation\n    cost =-(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                                        # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dw = (1 / m) *(np.dot(X,(A - Y).T))\n    db = (1 / m) *(np.sum(A - Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "alexandra.larsonneur@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A = sigmoid(np.dot(w.T,X) + b)                                     # compute activation\n    cost =-(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                                        # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dw = (1 / m) *(np.dot(X,(A - Y).T))\n    db = (1 / m) *(np.sum(A - Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-03 15:10:36",
        "atype": "bkcode"
    },
    "alexis.akujuobi-asoluka@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(np.dot(w.T,X)+b)                             # compute activation # A is a sigmoid function\n    cost =  (-1/m)*(np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1-A).T))                           # compute cost # writing of cost function\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*(np.dot(X,(A-Y).T))\n    db = (1/m)*(np.sum(A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "alexis.akujuobi-asoluka@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(np.dot(w.T,X)+b)                             # compute activation # A is a sigmoid function\n    cost =  (-1/m)*(np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1-A).T))                           # compute cost # writing of cost function\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*(np.dot(X,(A-Y).T))\n    db = (1/m)*(np.sum(A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 0.0,
        "update_time": "2023-10-03 19:24:01",
        "atype": "bkcode"
    },
    "angel.jouen@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n\n    A =   sigmoid(w.T@X + b)                            # compute activation\n    cost =  -1/(m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n\n    dw = 1/(m)*X@(A-Y).T\n    db = 1/(m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "angel.jouen@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n\n    A =   sigmoid(w.T@X + b)                            # compute activation\n    cost =  -1/(m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n\n    dw = 1/(m)*X@(A-Y).T\n    db = 1/(m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:00:51"
    },
    "antoine.rochette@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                       # compute activation\n    L = np.multiply(Y, np.log(A)) + np.multiply((1-Y), np.log(1-A))\n    cost = -1/m * np.sum(L)                               # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m*np.dot(X, (A-Y).T)\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "antoine.rochette@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                       # compute activation\n    L = np.multiply(Y, np.log(A)) + np.multiply((1-Y), np.log(1-A))\n    cost = -1/m * np.sum(L)                               # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m*np.dot(X, (A-Y).T)\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-03 15:18:37",
        "atype": "bkcode"
    },
    "arij.salablab@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cos\n    w_t = np.transpose(w)\n    A = sigmoid(np.dot(w_t, X) + b)                         # compute activation\n    cost = np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))*(-1/m)   # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*np.dot(X, np.transpose(A-Y)) \n    db = np.sum((1/m)*(A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "arij.salablab@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cos\n    w_t = np.transpose(w)\n    A = sigmoid(np.dot(w_t, X) + b)                         # compute activation\n    cost = np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))*(-1/m)   # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*np.dot(X, np.transpose(A-Y)) \n    db = np.sum((1/m)*(A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 21:32:46"
    },
    "armand.loisil@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid((np.dot(w.T,X))+b)                             # compute activation\n    cost =  -(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*np.dot(X,(A-Y).T)\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "armand.loisil@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid((np.dot(w.T,X))+b)                             # compute activation\n    cost =  -(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*np.dot(X,(A-Y).T)\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-03 15:10:43",
        "atype": "bkcode"
    },
    "axel.streiff@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =  sigmoid(np.dot(w.T,X) + b)  # compute activation\n    cost = (-1/m) * np.sum(np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1-A).T))\n    \"\"\"\n    cost = 0\n    for i in range(m):\n      cost += Y.T[i,0] * np.log(A.T[i,0]) + (1-Y.T[i,0]) * np.log(1-A.T[i,0])\n    cost =  float((-1/m) * cost)                        # compute cost\n    \"\"\"\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*(np.dot(X,(A-Y).T))\n    db = (1/m)*(np.sum(A-Y))\n    \"\"\"\n    dw = (1/m) * (X @ (A-Y).T)\n    db = 0\n    for i in range(m):\n      db += A.T[i] - Y.T[i]\n    db = (1/m) * db[0]\n    \"\"\"\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "axel.streiff@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =  sigmoid(np.dot(w.T,X) + b)  # compute activation\n    cost = (-1/m) * np.sum(np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1-A).T))\n    \"\"\"\n    cost = 0\n    for i in range(m):\n      cost += Y.T[i,0] * np.log(A.T[i,0]) + (1-Y.T[i,0]) * np.log(1-A.T[i,0])\n    cost =  float((-1/m) * cost)                        # compute cost\n    \"\"\"\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*(np.dot(X,(A-Y).T))\n    db = (1/m)*(np.sum(A-Y))\n    \"\"\"\n    dw = (1/m) * (X @ (A-Y).T)\n    db = 0\n    for i in range(m):\n      db += A.T[i] - Y.T[i]\n    db = (1/m) * db[0]\n    \"\"\"\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 15:48:20"
    },
    "badis.perdrix@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n   ### START CODE HERE ### Compute A and cost\n    wt = w.T\n    wt_X = np.dot(wt,X)\n    A =   sigmoid(wt_X+b)                             # compute activation\n    cost =  (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    X_new = np.dot(X, (A-Y).T)\n    dw = (1/m)*X_new\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "badis.perdrix@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n   ### START CODE HERE ### Compute A and cost\n    wt = w.T\n    wt_X = np.dot(wt,X)\n    A =   sigmoid(wt_X+b)                             # compute activation\n    cost =  (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    X_new = np.dot(X, (A-Y).T)\n    dw = (1/m)*X_new\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-04 15:54:17",
        "atype": "bkcode"
    },
    "baptiste.rousselet@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    Z = np.dot(w.T, X) + b\n    A = 1 / (1 + np.exp(-Z))  # Compute activation\n    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))  # Compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1 / m) * np.dot(X, (A - Y).T)\n    db = (1 / m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "baptiste.rousselet@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    Z = np.dot(w.T, X) + b\n    A = 1 / (1 + np.exp(-Z))  # Compute activation\n    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))  # Compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1 / m) * np.dot(X, (A - Y).T)\n    db = (1 / m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-04 10:44:05",
        "atype": "bkcode"
    },
    "colin.duchanoy@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(np.dot(w.T,X)+b)                              # compute activation\n    cost =  (-1/m) * np.sum(np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1 - A).T))         # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1 / m) *(np.dot(X,(A-Y).T))\n    db = (1 / m) *(np.sum(A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "colin.duchanoy@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(np.dot(w.T,X)+b)                              # compute activation\n    cost =  (-1/m) * np.sum(np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1 - A).T))         # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1 / m) *(np.dot(X,(A-Y).T))\n    db = (1 / m) *(np.sum(A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 15:22:30"
    },
    "dorian.rondeau@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A    = sigmoid(w.T@X+b)                             # compute activation\n    cost = (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m*X@(A-Y).T\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "dorian.rondeau@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A    = sigmoid(w.T@X+b)                             # compute activation\n    cost = (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m*X@(A-Y).T\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 10:45:56"
    },
    "evan.garcia@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(np.dot(w.T,X) + b)                             # compute activation\n    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * np.dot(X, (A - Y).T)\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "evan.garcia@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(np.dot(w.T,X) + b)                             # compute activation\n    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * np.dot(X, (A - Y).T)\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 20:19:54"
    },
    "gaetan.chiesura@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T @ X+b)   # compute activation\n\n    cost = -(1/m)*np.sum(Y*(np.log(A))+(1-Y)*(np.log(1-A)))    # compute cost\n    #cost = - (1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*X@(A-Y).T\n    db = (1/m)*np.sum((A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "gaetan.chiesura@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T @ X+b)   # compute activation\n\n    cost = -(1/m)*np.sum(Y*(np.log(A))+(1-Y)*(np.log(1-A)))    # compute cost\n    #cost = - (1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*X@(A-Y).T\n    db = (1/m)*np.sum((A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:09:43"
    },
    "geoffrey.vaillant@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)      # compute activation\n    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * np.dot(X, (A - Y).T)\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "geoffrey.vaillant@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)      # compute activation\n    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * np.dot(X, (A - Y).T)\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-04 13:29:32",
        "atype": "bkcode"
    },
    "john.doe@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.transpose(w)@X+b)                           # compute activation\n    cost = -(1/m)*np.sum(Y@np.log(A).T+(1-Y)@np.log(1-A).T)                     # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*X@np.transpose(A-Y)\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "john.doe@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.transpose(w)@X+b)                           # compute activation\n    cost = -(1/m)*np.sum(Y@np.log(A).T+(1-Y)@np.log(1-A).T)                     # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*X@np.transpose(A-Y)\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-04 10:45:35",
        "atype": "bkcode"
    },
    "julian.lavarelo@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T@X + b)\n                                # compute activation\n    cost =  0\n    for i in range(np.shape(A)[1]):\n      cost +=Y.T[i]*np.log(A.T[i])+ (1-Y.T[i])*np.log(1-A.T[i])\n    \n    \n    cost = float(-cost/m)                           # compute cost\n\n    ### END CODE HERE ###\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * X@(A-Y).T\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "julian.lavarelo@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T@X + b)\n                                # compute activation\n    cost =  0\n    for i in range(np.shape(A)[1]):\n      cost +=Y.T[i]*np.log(A.T[i])+ (1-Y.T[i])*np.log(1-A.T[i])\n    \n    \n    cost = float(-cost/m)                           # compute cost\n\n    ### END CODE HERE ###\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * X@(A-Y).T\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 6.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 10:46:47"
    },
    "lea.dupin@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T @ X + b)\n    cost = - (1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1 / m) * X @ (A - Y).T\n    db = (1 / m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "lea.dupin@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T @ X + b)\n    cost = - (1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1 / m) * X @ (A - Y).T\n    db = (1 / m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 10:44:08"
    },
    "luc.fourty@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                            # compute activation\n\n    cost = 0\n    for i in range(A.shape[0]):                           # compute cost\n      cost += np.dot(Y[i], np.log(A[i])) + np.dot((1 - Y[i]), np.log(1 - A[i]))\n    cost *= -(1/m)\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m) * X @ (A - Y).T\n    db = 0\n    for i in range(A.shape[1]):\n      db += (1/m) * (A[0][i] - Y[0][i])\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "luc.fourty@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                            # compute activation\n\n    cost = 0\n    for i in range(A.shape[0]):                           # compute cost\n      cost += np.dot(Y[i], np.log(A[i])) + np.dot((1 - Y[i]), np.log(1 - A[i]))\n    cost *= -(1/m)\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m) * X @ (A - Y).T\n    db = 0\n    for i in range(A.shape[1]):\n      db += (1/m) * (A[0][i] - Y[0][i])\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 13:40:43"
    },
    "luc.sauleau@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                             # compute activation\n    cost = (-1/m) * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*np.dot(X, (A-Y).T)\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "luc.sauleau@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                             # compute activation\n    cost = (-1/m) * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*np.dot(X, (A-Y).T)\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 15:59:23"
    },
    "mathieu.klingler@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T,X) + b)                                       # compute activation\n    cost = (-1/m) * np.sum((np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1 - A).T)))  # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m) *(np.dot(X,(A - Y).T))\n    db = (1/m) *(np.sum(A - Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "mathieu.klingler@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T,X) + b)                                       # compute activation\n    cost = (-1/m) * np.sum((np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1 - A).T)))  # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m) *(np.dot(X,(A - Y).T))\n    db = (1/m) *(np.sum(A - Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 15:23:14"
    },
    "matteo.crosnier-de-bellaistre@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.transpose(w)@X+b)                           # compute activation\n    cost = -(1/m)*np.sum(Y@np.log(A).T+(1-Y)@np.log(1-A).T)                     # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*X@np.transpose(A-Y)\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "matteo.crosnier-de-bellaistre@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.transpose(w)@X+b)                           # compute activation\n    cost = -(1/m)*np.sum(Y@np.log(A).T+(1-Y)@np.log(1-A).T)                     # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m)*X@np.transpose(A-Y)\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 08:42:54"
    },
    "mikael.kealbert@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    print(m)\n    print(Y)\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =  sigmoid(w.T@X + b)\n    cost =  -(1/m) * np.sum(Y@np.log(A).T + (-Y + 1)@np.log(- A + 1).T)                         # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * X @ (A - Y).T\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "mikael.kealbert@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    print(m)\n    print(Y)\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =  sigmoid(w.T@X + b)\n    cost =  -(1/m) * np.sum(Y@np.log(A).T + (-Y + 1)@np.log(- A + 1).T)                         # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * X @ (A - Y).T\n    db = 1/m * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 00:30:14"
    },
    "nathan.heckmann@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    z = w.T @ X + b\n    A = sigmoid(z)               # compute activation\n    cost = - (1/m) * np.sum((Y * np.log(A) + (1 - Y) * np.log(1 - A)))      # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### Compute dw and db\n    dw = (1/m) * X @ (A - Y).T\n    db = (1/m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "nathan.heckmann@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    z = w.T @ X + b\n    A = sigmoid(z)               # compute activation\n    cost = - (1/m) * np.sum((Y * np.log(A) + (1 - Y) * np.log(1 - A)))      # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### Compute dw and db\n    dw = (1/m) * X @ (A - Y).T\n    db = (1/m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-04 14:58:42",
        "atype": "bkcode"
    },
    "nils.joanne@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T@X+b)                          # compute activation\n    cost = -1/m*np.sum(Y@ np.log(A).T + (1-Y) @ np.log(1-A).T)                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m*X@(A-Y).T\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "nils.joanne@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(w.T@X+b)                          # compute activation\n    cost = -1/m*np.sum(Y@ np.log(A).T + (1-Y) @ np.log(1-A).T)                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m*X@(A-Y).T\n    db = 1/m*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-03 15:05:35",
        "atype": "bkcode"
    },
    "riccardo.cecchetto@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(w.T@X +b)                       # compute activation\n    cost =  -1/m*np.sum(Y @ np.log(A).T+ (1-Y)@np.log(1-A).T)     # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * X @ (A-Y).T\n    db = 1/m * np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "riccardo.cecchetto@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    A =   sigmoid(w.T@X +b)                       # compute activation\n    cost =  -1/m*np.sum(Y @ np.log(A).T+ (1-Y)@np.log(1-A).T)     # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * X @ (A-Y).T\n    db = 1/m * np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-03 15:07:05",
        "atype": "bkcode"
    },
    "romain.miaux@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    z = w.T @ X + b\n    A = sigmoid(z)               # compute activation\n    cost = (-1/m) * np.sum((Y * np.log(A) + (1 - Y) * np.log(1 - A)))      # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE\n    ### Compute dw and db\n    dw = (1/m) * X @ (A - Y).T\n    db = (1/m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1., 2., -1.],[3., 4., -3.2]]), np.array([[1, 0, 1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "romain.miaux@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    z = w.T @ X + b\n    A = sigmoid(z)               # compute activation\n    cost = (-1/m) * np.sum((Y * np.log(A) + (1 - Y) * np.log(1 - A)))      # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE\n    ### Compute dw and db\n    dw = (1/m) * X @ (A - Y).T\n    db = (1/m) * np.sum(A - Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1., 2., -1.],[3., 4., -3.2]]), np.array([[1, 0, 1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:05:33"
    },
    "solution": {
        "update_time": "2023-10-10 22:58:50",
        "visible": true,
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                           # compute activation\n    cost = -np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)) / m       # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = X@(A-Y).T/m\n    db = np.sum(A-Y)/m\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint(f\"dw = {grads['dw']}\")\nprint(f\"db = {grads['db']}\")\nprint(f\"cost = {cost}\")\n\n# il faudrait évaluer A, cost et dw et db si possible.\n",
        "note_upd": "2023-10-11 08:22:32",
        "explanation": "",
        "evaluation": "def student_evaluation_function(debug=False):\n  w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\n  score = bulkhours.is_equal(data_test=student.propagate(w,b,X,Y)[0][\"dw\"], max_score=3.0, data_ref=teacher.propagate(w,b,X,Y)[0][\"dw\"], min_score=0)\n\n  score += bulkhours.is_equal(data_test=student.propagate(w,b,X,Y)[0][\"db\"], max_score=3.0, data_ref=teacher.propagate(w,b,X,Y)[0][\"db\"], min_score=0)\n\n  score += bulkhours.is_equal(data_test=student.propagate(w,b,X,Y)[1], max_score=4.0, data_ref=teacher.propagate(w,b,X,Y)[1], min_score=0)\n\n  return score\n",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    ### START CODE HERE ### Compute A and cost\n    A = sigmoid(np.dot(w.T, X) + b)                           # compute activation\n    cost = -np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)) / m       # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = X@(A-Y).T/m\n    db = np.sum(A-Y)/m\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint(f\"dw = {grads['dw']}\")\nprint(f\"db = {grads['db']}\")\nprint(f\"cost = {cost}\")\n\n# il faudrait évaluer A, cost et dw et db si possible.\n",
        "user": "solution",
        "note": 10.0,
        "atype": "bkcode",
        "hint": ""
    },
    "sualp.komurcuoglu@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    Z = np.dot(w.T, X) + b\n    A = sigmoid(Z)  # Compute activation\n    \n    cost = -1/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)) # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * np.dot(X, (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "sualp.komurcuoglu@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    Z = np.dot(w.T, X) + b\n    A = sigmoid(Z)  # Compute activation\n    \n    cost = -1/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)) # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = 1/m * np.dot(X, (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-04 13:45:56",
        "atype": "bkcode"
    },
    "thibaud.bonnet@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    wX = np.dot(w.T, X)\n    A = sigmoid(wX + b)                         # compute activation\n    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    diff = A - Y\n    difft = diff.T\n    dw = 1 / m * np.dot(X, difft)\n    db = 1 / m * np.sum(diff)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "thibaud.bonnet@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    wX = np.dot(w.T, X)\n    A = sigmoid(wX + b)                         # compute activation\n    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    diff = A - Y\n    difft = diff.T\n    dw = 1 / m * np.dot(X, difft)\n    db = 1 / m * np.sum(diff)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-03 15:05:49",
        "atype": "bkcode"
    },
    "wassil.amghar@ipsa.fr": {
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    wt = w.T\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    wtX = np.dot(wt,X)\n    A =   sigmoid(wtX+b)                             # compute activation\n    cost =  (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    XAY = np.dot(X, (A-Y).T)\n    dw = (1/m)*XAY\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note_upd": "2023-10-11 08:22:32",
        "user": "wassil.amghar@ipsa.fr",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n    wt = w.T\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    wtX = np.dot(wt,X)\n    A =   sigmoid(wtX+b)                             # compute activation\n    cost =  (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                           # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    XAY = np.dot(X, (A-Y).T)\n    dw = (1/m)*XAY\n    db = (1/m)*np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "note": 10.0,
        "update_time": "2023-10-04 15:51:36",
        "atype": "bkcode"
    },
    "yann-loic-atasse.atakoui@ipsa.fr": {
        "note_upd": "2023-10-11 08:22:32",
        "note": 10.0,
        "answer": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    #w = np.random.uniform(num_px * num_px * 3, 1)\n    #A =   0                             # compute activation\n    A = sigmoid(w.T@X+b)\n    #cost =  (-1/m) * np.sum(y * np.log(a) + (1 - y)*np.log(1 - a))                           # compute cost\n    cost = -(1/m)*np.sum(Y*(np.log(A))+(1-Y)*(np.log(1-A)))    # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m) *X@(A-Y).T\n    db = (1/m) * np.sum((A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "main_execution": "def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n\n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n\n    m = X.shape[1]\n\n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### Compute A and cost\n    #w = np.random.uniform(num_px * num_px * 3, 1)\n    #A =   0                             # compute activation\n    A = sigmoid(w.T@X+b)\n    #cost =  (-1/m) * np.sum(y * np.log(a) + (1 - y)*np.log(1 - a))                           # compute cost\n    cost = -(1/m)*np.sum(Y*(np.log(A))+(1-Y)*(np.log(1-A)))    # compute cost\n    ### END CODE HERE ###\n\n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### Compute dw and db\n    dw = (1/m) *X@(A-Y).T\n    db = (1/m) * np.sum((A-Y))\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    assert(isinstance(cost, float))\n\n    grads = {\"dw\": dw, \"db\": db}\n\n    return grads, cost\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n",
        "user": "yann-loic-atasse.atakoui@ipsa.fr ",
        "atype": "bkcode",
        "update_time": "2023-10-04 13:44:26"
    }
}