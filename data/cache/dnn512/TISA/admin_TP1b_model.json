{
    "alexandra.larsonneur@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim=num_px*num_px*3\n    w, b =initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim=num_px*num_px*3\n    w, b =initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "alexandra.larsonneur@ipsa.fr",
        "update_time": "2023-10-03 15:26:43",
        "atype": "bkcode"
    },
    "alexis.akujuobi-asoluka@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0]) #previous function does it with random number\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w,b,X_train,Y_train, num_iterations,learning_rate,print_cost) # fixed.\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # fixed\n    Y_prediction_train = predict(w,b,X_train) # fixed\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0]) #previous function does it with random number\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w,b,X_train,Y_train, num_iterations,learning_rate,print_cost) # fixed.\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # fixed\n    Y_prediction_train = predict(w,b,X_train) # fixed\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "alexis.akujuobi-asoluka@ipsa.fr",
        "update_time": "2023-10-03 20:07:11",
        "atype": "bkcode"
    },
    "ali.m-sahi@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    w, b = initialize_weights_randomly(num_px*num_px*3) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    w, b = initialize_weights_randomly(num_px*num_px*3) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "ali.m-sahi@ipsa.fr",
        "update_time": "2023-10-04 14:14:34",
        "atype": "bkcode"
    },
    "angel.jouen@ipsa.fr": {
        "user": "angel.jouen@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w,b,X_train,Y_train, num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n  \n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w,b,X_train,Y_train, num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n  \n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "atype": "bkcode",
        "update_time": "2023-10-04 10:22:40"
    },
    "antoine.rochette@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim = np.shape(X_train)[0]\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim = np.shape(X_train)[0]\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "antoine.rochette@ipsa.fr",
        "update_time": "2023-10-03 15:48:50",
        "atype": "bkcode"
    },
    "arij.salablab@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0]) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0]) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "arij.salablab@ipsa.fr",
        "update_time": "2023-10-04 16:11:29",
        "atype": "bkcode"
    },
    "armand.loisil@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(3*num_px**2) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(3*num_px**2) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "armand.loisil@ipsa.fr",
        "update_time": "2023-10-03 15:26:17",
        "atype": "bkcode"
    },
    "axel.streiff@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(num_px * num_px * 3) # Fix this line...\n    \n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate,print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(num_px * num_px * 3) # Fix this line...\n    \n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate,print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "axel.streiff@ipsa.fr",
        "update_time": "2023-10-04 15:49:23",
        "atype": "bkcode"
    },
    "badis.perdrix@ipsa.fr": {
        "user": "badis.perdrix@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    ### START CODE HERE ###\n    # Initialize parameters with zeros\n    w, b = np.zeros((X_train.shape[0], 1)), 0  # Fix this line...\n\n    # Gradient descent\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)  # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples\n    Y_prediction_test = predict(w, b, X_test)  # Fix this line...\n    Y_prediction_train = predict(w, b, X_train)  # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n    print(\"Train accuracy: {} %\".format(train_accuracy))\n    print(\"Test accuracy: {} %\".format(test_accuracy))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\": Y_prediction_train,\n         \"w\": w,\n         \"b\": b,\n         \"learning_rate\": learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(15, 9))\n    for index in range(15):\n        row, col = index % 5, index // 5\n        if index == 14:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        else:\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(f\"{classes[yhat].decode('utf-8')} in the pix_{index} (y={y}, yhat={yhat})\", fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    ### START CODE HERE ###\n    # Initialize parameters with zeros\n    w, b = np.zeros((X_train.shape[0], 1)), 0  # Fix this line...\n\n    # Gradient descent\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)  # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples\n    Y_prediction_test = predict(w, b, X_test)  # Fix this line...\n    Y_prediction_train = predict(w, b, X_train)  # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n    print(\"Train accuracy: {} %\".format(train_accuracy))\n    print(\"Test accuracy: {} %\".format(test_accuracy))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\": Y_prediction_train,\n         \"w\": w,\n         \"b\": b,\n         \"learning_rate\": learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(15, 9))\n    for index in range(15):\n        row, col = index % 5, index // 5\n        if index == 14:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        else:\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(f\"{classes[yhat].decode('utf-8')} in the pix_{index} (y={y}, yhat={yhat})\", fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "atype": "bkcode",
        "update_time": "2023-10-04 18:43:19"
    },
    "baptiste.rousselet@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x_flatten, train_set_y, test_set_x_flatten, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x_flatten.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x_flatten[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x_flatten, train_set_y, test_set_x_flatten, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x_flatten.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x_flatten[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "baptiste.rousselet@ipsa.fr",
        "update_time": "2023-10-04 11:15:59",
        "atype": "bkcode"
    },
    "colin.duchanoy@ipsa.fr": {
        "user": "colin.duchanoy@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(3*num_px**2)\n\n    # Gradient descent from previous function\n    parameters, grads, costs =  optimize(w , b , X_train , Y_train , num_iterations , learning_rate , print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(3*num_px**2)\n\n    # Gradient descent from previous function\n    parameters, grads, costs =  optimize(w , b , X_train , Y_train , num_iterations , learning_rate , print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "atype": "bkcode",
        "update_time": "2023-10-03 15:24:31"
    },
    "dorian.rondeau@ipsa.fr": {
        "user": "dorian.rondeau@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim    = num_px*num_px*3\n    w, b   = initialize_weights_randomly(dim)\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3))) #transformation en image\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim    = num_px*num_px*3\n    w, b   = initialize_weights_randomly(dim)\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3))) #transformation en image\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "atype": "bkcode",
        "update_time": "2023-10-04 10:46:41"
    },
    "evan.garcia@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "evan.garcia@ipsa.fr",
        "update_time": "2023-10-04 10:58:16",
        "atype": "bkcode"
    },
    "gaetan.chiesura@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = False)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!= (1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = False)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!= (1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "gaetan.chiesura@ipsa.fr",
        "update_time": "2023-10-04 16:10:02",
        "atype": "bkcode"
    },
    "geoffrey.vaillant@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "geoffrey.vaillant@ipsa.fr",
        "update_time": "2023-10-04 13:39:36",
        "atype": "bkcode"
    },
    "john.doe@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = False)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = False)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "john.doe@ipsa.fr",
        "update_time": "2023-10-04 11:50:41",
        "atype": "bkcode"
    },
    "julian.lavarelo@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n\n    dim = np.shape(X_train)[0]\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    print(np.shape(train_set_x.T))\n    print(np.shape(train_set_y))\n    print(np.shape(test_set_x.T))\n    print(np.shape(test_set_y))\n  \n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            \n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n\n    dim = np.shape(X_train)[0]\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    print(np.shape(train_set_x.T))\n    print(np.shape(train_set_y))\n    print(np.shape(test_set_x.T))\n    print(np.shape(test_set_y))\n  \n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            \n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "julian.lavarelo@ipsa.fr",
        "update_time": "2023-10-04 10:49:09",
        "atype": "bkcode"
    },
    "lea.dupin@ipsa.fr": {
        "user": "lea.dupin@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = False)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = False)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "atype": "bkcode",
        "update_time": "2023-10-04 10:45:48"
    },
    "luc.fourty@ipsa.fr": {
        "user": "luc.fourty@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    w = w.T\n                                       \n    print(\"w.shape = (\", w.shape[0], \",\", w.shape[1])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    num_px = test_set_x_orig.shape[1]\n    dim = num_px * num_px * 3\n    w, b = initialize_weights_randomly(dim)\n\n    w = w.T\n                                       \n    print(\"w.shape = (\", w.shape[0], \",\", w.shape[1])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "atype": "bkcode",
        "update_time": "2023-10-04 15:52:32"
    },
    "luc.sauleau@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[1])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, Y_test)\n    Y_prediction_train = predict(w, b, Y_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[1])\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, Y_test)\n    Y_prediction_train = predict(w, b, Y_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "luc.sauleau@ipsa.fr",
        "update_time": "2023-10-03 16:05:17",
        "atype": "bkcode"
    },
    "mathieu.klingler@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(num_px*num_px*3) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train,num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(num_px*num_px*3) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train,num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "mathieu.klingler@ipsa.fr",
        "update_time": "2023-10-03 15:23:52",
        "atype": "bkcode"
    },
    "mikael.kealbert@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim = X_train.shape[0]\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    print(Y_test.shape)\n    print(Y_train.shape)\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim = X_train.shape[0]\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    print(Y_test.shape)\n    print(Y_train.shape)\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "mikael.kealbert@ipsa.fr",
        "update_time": "2023-10-04 00:30:29",
        "atype": "bkcode"
    },
    "nathan.heckmann@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim, _ = X_train.shape\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim, _ = X_train.shape\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "nathan.heckmann@ipsa.fr",
        "atype": "bkcode",
        "update_time": "2023-10-04 15:00:39"
    },
    "riccardo.cecchetto@ipsa.fr": {
        "user": "riccardo.cecchetto@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim, m_train = X_train.shape\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train , num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\n\nstudent_train_and_test()\n\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim, m_train = X_train.shape\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train , num_iterations, learning_rate, print_cost = False)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\n\nstudent_train_and_test()\n\n",
        "atype": "bkcode",
        "update_time": "2023-10-04 13:56:38"
    },
    "romain.miaux@ipsa.fr": {
        "user": "romain.miaux@ipsa.fr",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = True):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim, _ = X_train.shape\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy : {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = True):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    dim, _ = X_train.shape\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy : {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "atype": "bkcode",
        "update_time": "2023-10-04 16:06:03"
    },
    "sualp.komurcuoglu@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    # w, b = np.zeros((X_train.shape[0], 1)), 0 # Fix this line...\n    print(X_train.shape)\n    dim, m_train = X_train.shape\n\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    # w, b = np.zeros((X_train.shape[0], 1)), 0 # Fix this line...\n    print(X_train.shape)\n    dim, m_train = X_train.shape\n\n    w, b = initialize_weights_randomly(dim)\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w,b,X_test) # Fix this line...\n    Y_prediction_train = predict(w,b,X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "sualp.komurcuoglu@ipsa.fr",
        "update_time": "2023-10-04 17:55:32",
        "atype": "bkcode"
    },
    "thibaud.bonnet@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0]) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    ### START CODE HERE ###\n    # Initialize parameters with zeros from previous function\n    w, b = initialize_weights_randomly(X_train.shape[0]) # Fix this line...\n\n    # Gradient descent from previous function\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples from previous function\n    Y_prediction_test = predict(w, b, X_test) # Fix this line...\n    Y_prediction_train = predict(w, b, X_train) # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=(nrows:=3), ncols=(ncols:=5), figsize=(5*3, 3*3))\n    for index in range(nrows*ncols):\n        row, col = index % ncols, index // ncols\n        if index == nrows*ncols - 1:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        elif test_set_x.shape!=(1, 1):\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(\"%s in the pix_%s (y=%s, yhat=%s)\" % (classes[yhat].decode(\"utf-8\"), index, y, yhat), fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "thibaud.bonnet@ipsa.fr",
        "update_time": "2023-10-04 15:42:01",
        "atype": "bkcode"
    },
    "wassil.amghar@ipsa.fr": {
        "answer": "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    ### START CODE HERE ###\n    # Initialize parameters with zeros\n    w, b = np.zeros((X_train.shape[0], 1)), 0  # Fix this line...\n\n    # Gradient descent\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)  # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples\n    Y_prediction_test = predict(w, b, X_test)  # Fix this line...\n    Y_prediction_train = predict(w, b, X_train)  # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n    print(\"Train accuracy: {} %\".format(train_accuracy))\n    print(\"Test accuracy: {} %\".format(test_accuracy))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\": Y_prediction_train,\n         \"w\": w,\n         \"b\": b,\n         \"learning_rate\": learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\n    \ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(15, 9))\n    for index in range(15):\n        row, col = index % 5, index // 5\n        if index == 14:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        else:\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(f\"{classes[yhat].decode('utf-8')} in the pix_{index} (y={y}, yhat={yhat})\", fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "main_execution": "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n\n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n\n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    ### START CODE HERE ###\n    # Initialize parameters with zeros\n    w, b = np.zeros((X_train.shape[0], 1)), 0  # Fix this line...\n\n    # Gradient descent\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)  # Fix this line...\n\n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    # Predict test/train set examples\n    Y_prediction_test = predict(w, b, X_test)  # Fix this line...\n    Y_prediction_train = predict(w, b, X_train)  # Fix this line...\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n    test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n    print(\"Train accuracy: {} %\".format(train_accuracy))\n    print(\"Test accuracy: {} %\".format(test_accuracy))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\": Y_prediction_train,\n         \"w\": w,\n         \"b\": b,\n         \"learning_rate\": learning_rate,\n         \"num_iterations\": num_iterations}\n\n    return d\n\n    \ndef student_train_and_test():\n    d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n    train_acc = 100 - np.mean(np.abs(d['Y_prediction_train'] - train_set_y)) * 100\n    test_acc = 100 - np.mean(np.abs(d['Y_prediction_test'] - test_set_y)) * 100\n    fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(15, 9))\n    for index in range(15):\n        row, col = index % 5, index // 5\n        if index == 14:\n            # Plot learning curve (with costs)\n            ax[col][row].plot(np.squeeze(d['costs']))\n            ax[col][row].set_ylabel('cost', fontsize=9)\n            ax[col][row].set_xlabel('iterations (per hundreds)', fontsize=9)\n            ax[col][row].set_title(f\"Learning rate={d['learning_rate']}\\nTrain={train_acc:.2f}%, Test={test_acc:.2f}%\", fontsize=10)\n        else:\n            ax[col][row].imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n            y, yhat = test_set_y[0][index], int(d[\"Y_prediction_test\"][0][index])\n            ax[col][row].set_title(f\"{classes[yhat].decode('utf-8')} in the pix_{index} (y={y}, yhat={yhat})\", fontsize=10, color=\"green\" if y == yhat else \"red\")\n            ax[col][row].set_axis_off()\n\n# Run the following function to train your model.\nstudent_train_and_test()\n",
        "user": "wassil.amghar@ipsa.fr",
        "update_time": "2023-10-04 18:42:52",
        "atype": "bkcode"
    }
}