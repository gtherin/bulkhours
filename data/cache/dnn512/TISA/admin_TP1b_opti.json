{
    "alexandra.larsonneur@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "alexandra.larsonneur@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-03 15:14:39",
        "atype": "bkcode"
    },
    "alexis.akujuobi-asoluka@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)  # Done, using propagate function\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w-learning_rate\n        b = b-learning_rate\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n\n# can't see expected output due to error in original code, can't know if false, to correct\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "alexis.akujuobi-asoluka@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)  # Done, using propagate function\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w-learning_rate\n        b = b-learning_rate\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n\n# can't see expected output due to error in original code, can't know if false, to correct\n",
        "note": 2.0,
        "update_time": "2023-10-03 19:36:16",
        "atype": "bkcode"
    },
    "ali.m-sahi@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw  # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "ali.m-sahi@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw  # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-04 13:41:30",
        "atype": "bkcode"
    },
    "angel.jouen@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w- learning_rate*dw\n        b = b- learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "angel.jouen@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w- learning_rate*dw\n        b = b- learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:01:04"
    },
    "antoine.rochette@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "antoine.rochette@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-03 15:23:58",
        "atype": "bkcode"
    },
    "arij.salablab@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - dw*learning_rate # Fix this line...\n        b = b - db*learning_rate  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "arij.salablab@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - dw*learning_rate # Fix this line...\n        b = b - db*learning_rate  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 21:32:51"
    },
    "armand.loisil@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw  # Fix this line...\n        b = b - learning_rate * db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "armand.loisil@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw  # Fix this line...\n        b = b - learning_rate * db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-03 15:14:40",
        "atype": "bkcode"
    },
    "axel.streiff@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw  # Fix this line...\n        b = b - learning_rate * db # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "axel.streiff@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw  # Fix this line...\n        b = b - learning_rate * db # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 15:48:32"
    },
    "badis.perdrix@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y) # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w -= learning_rate * dw # Fix this line...\n        b -= learning_rate * db # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "badis.perdrix@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y) # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w -= learning_rate * dw # Fix this line...\n        b -= learning_rate * db # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-04 17:45:09",
        "atype": "bkcode"
    },
    "baptiste.rousselet@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # 1) Cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # 2) Update parameters\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "baptiste.rousselet@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # 1) Cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # 2) Update parameters\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-04 11:06:32",
        "atype": "bkcode"
    },
    "colin.duchanoy@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "colin.duchanoy@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 15:22:48"
    },
    "dorian.rondeau@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w-learning_rate*dw\n        b = b-learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "dorian.rondeau@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w-learning_rate*dw\n        b = b-learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 10:46:19"
    },
    "evan.garcia@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "evan.garcia@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 10:57:39"
    },
    "gaetan.chiesura@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost=propagate(w,b,X,Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w -= learning_rate*dw\n        b -= learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "gaetan.chiesura@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost=propagate(w,b,X,Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w -= learning_rate*dw\n        b -= learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 10.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:09:50"
    },
    "geoffrey.vaillant@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "geoffrey.vaillant@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-04 13:31:50",
        "atype": "bkcode"
    },
    "john.doe@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y) # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw # Fix this line...\n        b = b -  learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "john.doe@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y) # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw # Fix this line...\n        b = b -  learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 10.0,
        "update_time": "2023-10-04 10:45:56",
        "atype": "bkcode"
    },
    "julian.lavarelo@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw  # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "julian.lavarelo@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw  # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-04 10:46:58",
        "atype": "bkcode"
    },
    "lea.dupin@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w -= learning_rate * dw\n        b -= learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "lea.dupin@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w -= learning_rate * dw\n        b -= learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 10:44:16"
    },
    "luc.fourty@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db    # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "luc.fourty@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db    # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 13:41:05"
    },
    "luc.sauleau@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "luc.sauleau@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw\n        b = b - learning_rate*db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-03 15:34:40",
        "atype": "bkcode"
    },
    "mathieu.klingler@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w-learning_rate*dw  # Fix this line...\n        b = b-learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "mathieu.klingler@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w,b,X,Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w-learning_rate*dw  # Fix this line...\n        b = b-learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-03 15:23:09"
    },
    "matteo.crosnier-de-bellaistre@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y) # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw # Fix this line...\n        b = b -  learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "matteo.crosnier-de-bellaistre@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y) # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw # Fix this line...\n        b = b -  learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 08:43:30"
    },
    "nathan.heckmann@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "nathan.heckmann@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-04 14:58:48",
        "atype": "bkcode"
    },
    "nils.joanne@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw  # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "nils.joanne@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw  # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-03 15:05:29",
        "atype": "bkcode"
    },
    "riccardo.cecchetto@ipsa.fr": {
        "answer": "\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost =  propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "riccardo.cecchetto@ipsa.fr",
        "main_execution": "\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost =  propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate*dw # Fix this line...\n        b = b - learning_rate*db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 10.0,
        "update_time": "2023-10-03 15:19:13",
        "atype": "bkcode"
    },
    "romain.miaux@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "romain.miaux@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "atype": "bkcode",
        "update_time": "2023-10-04 16:05:40"
    },
    "solution": {
        "update_time": "2023-10-10 23:01:35",
        "visible": true,
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\n\n",
        "note_upd": "2023-10-11 08:23:20",
        "explanation": "",
        "evaluation": "def student_evaluation_function():\n  score = bulkhours.is_equal(data_test=student.params[\"w\"], max_score=2.0, data_ref=teacher.params[\"w\"], min_score=0)\n\n  score += bulkhours.is_equal(data_test=student.params[\"b\"], max_score=2.0, data_ref=teacher.params[\"b\"], min_score=0)\n\n  score += bulkhours.is_equal(data_test=student.grads[\"dw\"], max_score=2.0, data_ref=teacher.grads[\"dw\"], min_score=0)\n\n  score += bulkhours.is_equal(data_test=student.grads[\"db\"], max_score=2.0, data_ref=teacher.grads[\"db\"], min_score=0)\n\n  score += bulkhours.is_equal(data_test=student.costs, max_score=2.0, data_ref=teacher.costs, min_score=0)\n\n  return score\n",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nw, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]) # Simultaneous assignation of four variables!\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\n\n",
        "user": "solution",
        "note": 10.0,
        "atype": "bkcode",
        "hint": ""
    },
    "sualp.komurcuoglu@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        # Update rule\n        w = w - learning_rate * dw  # weights\n        b = b - learning_rate * db  # bias\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "sualp.komurcuoglu@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        # Update rule\n        w = w - learning_rate * dw  # weights\n        b = b - learning_rate * db  # bias\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 10.0,
        "update_time": "2023-10-04 13:55:41",
        "atype": "bkcode"
    },
    "thibaud.bonnet@ipsa.fr": {
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw  # Fix this line...\n        b = b - learning_rate * db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note_upd": "2023-10-11 08:23:20",
        "user": "thibaud.bonnet@ipsa.fr",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)  # Fix this line...\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw  # Fix this line...\n        b = b - learning_rate * db  # Fix this line...\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "note": 0.0,
        "update_time": "2023-10-03 15:16:39",
        "atype": "bkcode"
    },
    "yann-loic-atasse.atakoui@ipsa.fr": {
        "note_upd": "2023-10-11 08:23:20",
        "note": 10.0,
        "answer": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "main_execution": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n\n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n\n    costs = []\n\n    for i in range(num_iterations):\n        # Cost and gradient calculation from previous function\n        grads, cost = propagate(w, b, X, Y)\n\n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n\n        # Print the cost every 100 training iterations\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n\n    params = {\"w\": w, \"b\": b}\n    grads = {\"dw\": dw, \"db\": db}\n\n    return params, grads, costs\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint(\"w = \" + str(params[\"w\"]))\nprint(\"b = \" + str(params[\"b\"]))\nprint(\"dw = \" + str(grads[\"dw\"]))\nprint(\"db = \" + str(grads[\"db\"]))\n",
        "user": "yann-loic-atasse.atakoui@ipsa.fr ",
        "update_time": "2023-10-04 13:46:54",
        "atype": "bkcode"
    }
}